# ROLAND
This repository contains the adaptation we made of ROLAND method from this [paper](https://arxiv.org/abs/2208.07239). This repository is strongly based on ROLAND's [repository](https://github.com/snap-stanford/roland), which is itself a version of [GraphGym](https://github.com/snap-stanford/GraphGym) modified for the ROLAND paper to include their new model implementation.

## Our Modifications
We list here the main modifications we made to the original ROLAND repository:

1. We added a new data-loader for our kind of dynamic graphs: `graphgym/contrib/loader/timestep_dataset.py`
2. We modified the live-update train mechanism to generate node embeddings at each timestep: `graphgym/contrib/train/train_live_update.py`
3. We modified the main file in order to run the algorithm on ALL listed dataset by default: `run/main.py`
4. We added a string command line argument `--single` to decide whether to run the algorithm on just one dataset instead (default `no`): `graphgym/cmd_args.py`

## Environment Setup
All the essential libraries are listed in the `requirements.txt` file. To install them, you can use the following command:

```bash
pip install -r requirements.txt
```

## Run The Code
In all our experiments we used the yaml file located in `run/roland_example.yaml` to set the hyperparameters of the model.

To run the code, you can use the following command:

```bash
cd run
python main.py --cfg ./roland_example.yaml --repeat 1
```

## `yaml` Configuration File Description
Here we provide an explanation of the yaml configuration file.

```yaml
remark: Youtube_setting1  # embeddings are saved in 'run/runs_<remark>' (important just when --single=True)
out_dir: grid_search__moveavg_0_2  # directory to save the results
device: auto  # {'cpu', 'gpu', 'auto'}
experimental:
  rank_eval_multiplier: 100
dataset:
  format: timestep_dataset  # Set which dataloader to use
  name: Youtube_setting1 
  is_hetero: False
  dir: ../DynGEM_CTGCN/data/Youtube_setting1/1.format  # directory of the dataset (important just whn --single=True)
  task: link_pred
  shuffle: True
  negative_sample_weight: uniform
  task_type: classification
  transductive: True
  split: [0.8, 0.1, 0.1]
  augment_feature: []
  augment_feature_dims: [0]
  augment_feature_repr: position
  augment_label: ''
  augment_label_dims: 0
  transform: none
  edge_encoder: True  # CONTROLLED_IN_GRID
  edge_dim: 1  # CONTROLLED_IN_GRID, edge_dim in the raw dataset.
  edge_encoder_name: roland_general
  edge_encoder_bn: True
  node_encoder: True
  node_encoder_name: roland
  node_encoder_bn: True
transaction:
  keep_ratio: linear
  snapshot: True
  snapshot_freq: D
  check_snapshot: False
  history: rolling
  horizon: 1
  pred_mode: at
  loss: supervised
  feature_int_dim: 16  # only used when one of init_num != [].
  feature_edge_int_num: []  #
  feature_node_int_num: [1483, 32, 13, 24, 5]  # loader will set this if needed.
  feature_amount_dim: 16
  feature_time_dim: 16
train:
  batch_size: 32
  eval_period: 20
  ckpt_period: 400
  mode: live_update
model:
  type: gnn_recurrent
  loss_fun: cross_entropy
  edge_decoding: concat
  graph_pooling: add
gnn:
  embed_update_method: moving_average  # which function to use for the embedding UPDATE {'mlp', 'moving_average', 'gru'}
  layers_pre_mp: 1
  layers_mp: 2
  layers_post_mp: 2
  dim_inner: 64  # dimension of output embeddings
  layer_type: residual_edge_conv
  skip_connection: affine
  stage_type: stack
  batchnorm: True
  act: prelu
  dropout: 0.0  # dropout rate inside GNN
  agg: add
  att_heads: 2  # number of attention heads
  normalize_adj: False
optim:
  optimizer: adam
  base_lr: 0.03
  max_epoch: 100
meta:
  alpha: 0.9
  is_meta: false
  method: moving_average
```

## Remark
With this code we only generate node embeddings (using ROLAND), we do not perform community detection. To perform community detection, we need to use the embeddings generated by ROLAND and use them as input to a clustering algorithm. We used the embeddings generated by ROLAND as input to the K-means algorithm. We provide the code to perform this task in the `k_means_clustering.py` file, which is inside the folder `DynGEM_CTGCN`.
